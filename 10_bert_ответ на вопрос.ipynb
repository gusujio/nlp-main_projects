{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QPPvH6jbQX5"
      },
      "source": [
        "## BERT для вопросно-ответных систем\n",
        "\n",
        "На этом семинаре мы продолжим работать с BERT и применим его для решения более сложной задачи — а именно, мы научим BERT отвечать на вопросы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coi25H-wbQX6",
        "outputId": "8a340efa-6528-4066-cda8-8386198d3d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stepik-dl-nlp'...\n",
            "remote: Enumerating objects: 289, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 289 (delta 10), reused 14 (delta 6), pack-reused 266\u001b[K\n",
            "Receiving objects: 100% (289/289), 42.27 MiB | 16.33 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (1.0.2)\n",
            "Collecting spacy-udpipe\n",
            "  Downloading spacy_udpipe-1.0.0-py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (1.10.0+cu111)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.2.2)\n",
            "Collecting ipymarkup\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (4.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.62.3)\n",
            "Collecting youtokentome\n",
            "  Downloading youtokentome-1.0.6-cp37-cp37m-manylinux2010_x86_64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 13.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.11.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (4.10.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (5.5.0)\n",
            "Collecting pyconll\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 53.0 MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Collecting livelossplot==0.5.3\n",
            "  Downloading livelossplot-0.5.3-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.15.0)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.10.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (3.1.0)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "  Downloading ufal.udpipe-1.2.0.3.tar.gz (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 56.1 MB/s \n",
            "\u001b[?25hCollecting spacy<4.0.0,>=3.0.0\n",
            "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 26.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (57.4.0)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.9.0)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (21.3)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 60.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.6)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 43.3 MB/s \n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 82.3 MB/s \n",
            "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.6)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (7.1.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.11.0)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.5)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (3.13)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (22.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.9.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.0)\n",
            "Building wheels for collected packages: ufal.udpipe, intervaltree, wget\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626644 sha256=888426a93db0a8df258022aa218c206d9e770b912a1544e266dce3d6e4dedc4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/b5/8e/3da091629a21ce2d10bf90759d0cb034ba10a5cf7a01e83d64\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26119 sha256=e86776604d07d670c17129dc0adeb82618060dffe79c2894c065852827ad8c3c\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/85/bd/1001cbb46dcfb71c2001cd7401c6fb250392f22a81ce3722f7\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=e7b139705717bae90aa1967c311cb10690b03ae4c55c858e4063c488a7553636\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built ufal.udpipe intervaltree wget\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, ufal.udpipe, spacy, pymorphy2-dicts-ru, intervaltree, dawg-python, youtokentome, wget, spacy-udpipe, pymorphy2, pyconll, livelossplot, ipymarkup, gensim\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: intervaltree\n",
            "    Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed catalogue-2.0.6 dawg-python-0.7.2 gensim-3.8.1 intervaltree-3.1.0 ipymarkup-0.9.0 langcodes-3.3.0 livelossplot-0.5.3 pathy-0.6.1 pyconll-3.1.0 pydantic-1.8.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 spacy-udpipe-1.0.0 srsly-2.4.2 thinc-8.0.13 typer-0.4.0 ufal.udpipe-1.2.0.3 wget-3.2 youtokentome-1.0.6\n"
          ]
        }
      ],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle,\n",
        "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
        "\n",
        "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "import sys; sys.path.append('./stepik-dl-nlp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA3SgecCbQX7"
      },
      "source": [
        "Скачайте датасет (SQuAD) [отсюда](https://rajpurkar.github.io/SQuAD-explorer/). Для выполенения семинара Вам понадобятся файлы `train-v2.0.json` и `dev-v2.0.json`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foxlhFnabQX8"
      },
      "source": [
        "Склонируйте репозиторий https://github.com/huggingface/transformers (воспользуйтесь скриптом `clone_pytorch_transformers.sh`) и положите путь до папки `examples` в переменную `PATH_TO_EXAMPLES`. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Qx2QQklOgquj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Так же как и в предыдущем семинаре, мы будем использовать библиотеку pytorch-transformers. Мы, опять же, не будем писать самостоятельно сеть для решения задачи понимания текста и ответа на вопросы по нему, а научимся работать со скриптами из pytorch-transformers."
      ],
      "metadata": {
        "id": "ZKt3B71ygrxA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoXvLUvYbQX8"
      },
      "outputs": [],
      "source": [
        "PATH_TO_TRANSFORMERS_REPO = '../transformers/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T-Vc7ImbQX8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PATH_TO_TRANSFORMER_REPO'] = PATH_TO_TRANSFORMERS_REPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hOij9VjbQX8"
      },
      "outputs": [],
      "source": [
        "! bash clone_pytorch_transformers.sh $PATH_TO_TRANSFORMERS_REPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqZnrbMebQX9"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "PATH_TO_EXAMPLES = os.path.join(PATH_TO_TRANSFORMERS_REPO, 'examples')\n",
        "sys.path.append(PATH_TO_EXAMPLES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "YGlrQl0TbQX9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tqdm\n",
        "import json\n",
        "\n",
        "from utils_squad import (read_squad_examples, convert_examples_to_features,\n",
        "                         RawResult, write_predictions,\n",
        "                         RawResultExtended, write_predictions_extended)\n",
        "\n",
        "from run_squad import train, load_and_cache_examples\n",
        "\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
        "\n",
        "from transformers import (WEIGHTS_NAME, BertConfig, XLNetConfig, XLMConfig,\n",
        "                          BertForQuestionAnswering, BertTokenizer)\n",
        "\n",
        "from utils_squad_evaluate import EVAL_OPTS, main as evaluate_on_squad\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnsc-wzubQX9"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH582bcqbQX-"
      },
      "outputs": [],
      "source": [
        "# если Вы не хотите запускать файн-тюнинг, пропустите блок \"Дообучение\",\n",
        "# подгрузите веса уже дообученной модели и переходите к блоку \"Оценка качества\"\n",
        "\n",
        "# скачайте веса с Google-диска и положите их в папку models\n",
        "# https://drive.google.com/drive/folders/1-DR30q7MF-gZ51TDx596dAOhgh-uOAPj?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Uy4sptZbQX-"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    model.load_state_dict(torch.load('models/bert_squad_1epoch.pt')) # если у вас есть GPU\n",
        "else:\n",
        "    model.load_state_dict(torch.load('models/bert_squad_1epoch.pt', map_location=device)) # если GPU нет"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcp7ds7vbQX-"
      },
      "source": [
        "### Дообучение\n",
        "\n",
        "основные параметры модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTYtR3WFbQX-"
      },
      "outputs": [],
      "source": [
        "# !pip install dataclasses\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TRAIN_OPTS:\n",
        "    train_file : str = 'train-v2.0.json'    # SQuAD json-файл для обучения\n",
        "    predict_file : str = 'dev-v2.0.json'    # SQuAD json-файл для тестирования\n",
        "    model_type : str = 'bert'               # тип модели (может быть  'bert', 'xlnet', 'xlm', 'distilbert')\n",
        "    model_name_or_path : str = 'bert-base-uncased' # путь до предобученной модели или название модели из ALL_MODELS\n",
        "    output_dir : str = '/tmp' # путь до директории, где будут храниться чекпоинты и предсказания модели\n",
        "    device : str = 'cuda' # cuda или cpu\n",
        "    n_gpu : int = 1 # количество gpu для обучения\n",
        "    cache_dir : str = '' # где хранить предобученные модели, загруженные с s3\n",
        "        \n",
        "    # Если true, то в датасет будут включены вопросы, на которые нет ответов.\n",
        "    version_2_with_negative : bool = True\n",
        "    # Если (null_score - best_non_null) больше, чем порог, предсказывать null.\n",
        "    null_score_diff_threshold : float = 0.0\n",
        "    # Максимальная длина входной последовательности после WordPiece токенизации. Sequences \n",
        "    # Последовательности длиннее будут укорочены, для более коротких последовательностей будет использован паддинг\n",
        "    max_seq_length : int = 384\n",
        "    # Сколько stride использовать при делении длинного документа на чанки\n",
        "    doc_stride : int = 128\n",
        "    # Максимальное количество токенов в вопросе. Более длинные вопросы будут укорочены до этой длины\n",
        "    max_query_length : int = 128 #\n",
        "        \n",
        "    do_train : bool = True\n",
        "    do_eval : bool = True\n",
        "        \n",
        "    # Запускать ли evaluation на каждом logging_step\n",
        "    evaluate_during_training : bool = True\n",
        "    # Должно быть True, если Вы используете uncased модели\n",
        "    do_lower_case : bool = True #\n",
        "    \n",
        "    per_gpu_train_batch_size : int = 8 # размер батча для обучения\n",
        "    per_gpu_eval_batch_size : int = 8 # размер батча для eval\n",
        "    learning_rate : float = 5e-5 # learning rate\n",
        "    gradient_accumulation_steps : int = 1 # количество шагов, которые нужно сделать перед backward/update pass\n",
        "    weight_decay : float = 0.0 # weight decay\n",
        "    adam_epsilon : float = 1e-8 # эпсилон для Adam\n",
        "    max_grad_norm : float = 1.0 # максимальная норма градиента\n",
        "    num_train_epochs : float = 5.0 # количество эпох на обучение\n",
        "    max_steps : int = -1 # общее количество шагов на обучение (override num_train_epochs)\n",
        "    warmup_steps : int = 0 # warmup \n",
        "    n_best_size : int = 5 # количество ответов, которые надо сгенерировать для записи в nbest_predictions.json\n",
        "    max_answer_length : int = 30 # максимально возможная длина ответа\n",
        "    verbose_logging : bool = True # печатать или нет warnings, относящиеся к обработке данных\n",
        "    logging_steps : int = 5000 # логировать каждые X шагов\n",
        "    save_steps : int = 5000 # сохранять чекпоинт каждые X шагов\n",
        "        \n",
        "    # Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\n",
        "    eval_all_checkpoints : bool = True\n",
        "    no_cuda : bool = False # не использовать CUDA\n",
        "    overwrite_output_dir : bool = True # переписывать ли содержимое директории с выходными файлами\n",
        "    overwrite_cache : bool = True # переписывать ли закешированные данные для обучения и evaluation\n",
        "    seed : int = 42 # random seed\n",
        "    local_rank : int = -1 # local rank для распределенного обучения на GPU\n",
        "    fp16 : bool = False # использовать ли 16-bit (mixed) precision (через NVIDIA apex) вместо 32-bit\"\n",
        "    # Apex AMP optimization level: ['O0', 'O1', 'O2', and 'O3'].\n",
        "    # Подробнее тут: https://nvidia.github.io/apex/amp.html\n",
        "    fp16_opt_level : str = '01'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Takj3-ttbQX_"
      },
      "outputs": [],
      "source": [
        "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) \\\n",
        "                  for conf in (BertConfig, XLNetConfig, XLMConfig)), ())\n",
        "ALL_MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "FQXAuEpjbQYA"
      },
      "outputs": [],
      "source": [
        "args = TRAIN_OPTS()\n",
        "train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Zk2douSTbQYA"
      },
      "outputs": [],
      "source": [
        "train(args, train_dataset, model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM2LBH8xbQYA"
      },
      "source": [
        "Сохраняем веса дообученной модели на диск, чтобы в следующий раз не обучать модель заново."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPEY_fcabQYA"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'models/bert_squad_final_5epoch.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDkjSIN_bQYA"
      },
      "source": [
        "Подгрузить веса модели можно так:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk4zdKQLbQYB"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('models/bert_squad_5epochs.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfhFrTxVbQYB"
      },
      "source": [
        "### Оценка качества работы модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0MB6Z5pbQYB"
      },
      "outputs": [],
      "source": [
        "PATH_TO_DEV_SQUAD = 'dev-v2.0.json'\n",
        "PATH_TO_SMALL_DEV_SQUAD = 'small_dev-v2.0.json'\n",
        "\n",
        "with open(PATH_TO_DEV_SQUAD, 'r') as iofile:\n",
        "    full_sample = json.load(iofile)\n",
        "    \n",
        "small_sample = {\n",
        "    'version': full_sample['version'],\n",
        "    'data': full_sample['data'][:1]\n",
        "}\n",
        "\n",
        "with open(PATH_TO_SMALL_DEV_SQUAD, 'w') as iofile:\n",
        "    json.dump(small_sample, iofile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PzWAGrZbQYB"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 384\n",
        "outside_pos = max_seq_length + 10\n",
        "doc_stride = 128\n",
        "max_query_length = 64\n",
        "max_answer_length = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "885RTy_CbQYB"
      },
      "outputs": [],
      "source": [
        "examples = read_squad_examples(\n",
        "    input_file=PATH_TO_SMALL_DEV_SQUAD,\n",
        "    is_training=False,\n",
        "    version_2_with_negative=True)\n",
        "\n",
        "features = convert_examples_to_features(\n",
        "    examples=examples,\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=max_seq_length,\n",
        "    doc_stride=doc_stride,\n",
        "    max_query_length=max_query_length,\n",
        "    is_training=False,\n",
        "    cls_token_segment_id=0,\n",
        "    pad_token_segment_id=0,\n",
        "    cls_token_at_end=False\n",
        ")\n",
        "\n",
        "input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n",
        "p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n",
        "\n",
        "example_index = torch.arange(input_ids.size(0), dtype=torch.long)\n",
        "dataset = TensorDataset(input_ids, input_mask, segment_ids, example_index, cls_index, p_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq9WbRBCbQYC"
      },
      "outputs": [],
      "source": [
        "eval_sampler = SequentialSampler(dataset)\n",
        "eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLesV8YcbQYC"
      },
      "outputs": [],
      "source": [
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()\n",
        "\n",
        "all_results = []\n",
        "for idx, batch in enumerate(tqdm.tqdm_notebook(eval_dataloader, desc=\"Evaluating\")):\n",
        "    model.eval()\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    with torch.no_grad():\n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1]\n",
        "                  }\n",
        "        inputs['token_type_ids'] = batch[2]\n",
        "        example_indices = batch[3]\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    for i, example_index in enumerate(example_indices):\n",
        "        eval_feature = features[example_index.item()]\n",
        "        unique_id = int(eval_feature.unique_id)\n",
        "        result = RawResult(unique_id    = unique_id,\n",
        "                           start_logits = to_list(outputs[0][i]),\n",
        "                           end_logits   = to_list(outputs[1][i]))\n",
        "        all_results.append(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osUjWqYSbQYC"
      },
      "outputs": [],
      "source": [
        "all_results[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss5p0q0FbQYC"
      },
      "outputs": [],
      "source": [
        "n_best_size = 5\n",
        "do_lower_case = True\n",
        "output_prediction_file = 'output_1best_file'\n",
        "output_nbest_file = 'output_nbest_file'\n",
        "output_na_prob_file = 'output_na_prob_file'\n",
        "verbose_logging = True\n",
        "version_2_with_negative = True\n",
        "null_score_diff_threshold = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIh0a8uybQYC"
      },
      "outputs": [],
      "source": [
        "# Генерируем файл с n лучшими ответами `output_nbest_file`\n",
        "write_predictions(examples, features, all_results, n_best_size,\n",
        "                    max_answer_length, do_lower_case, output_prediction_file,\n",
        "                    output_nbest_file, output_na_prob_file, verbose_logging,\n",
        "                    version_2_with_negative, null_score_diff_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygMwWuqAbQYD"
      },
      "outputs": [],
      "source": [
        "# Считаем метрики используя официальный SQuAD script\n",
        "evaluate_options = EVAL_OPTS(data_file=PATH_TO_SMALL_DEV_SQUAD,\n",
        "                             pred_file=output_prediction_file,\n",
        "                             na_prob_file=output_na_prob_file)\n",
        "results = evaluate_on_squad(evaluate_options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeBHYmQdbQYD"
      },
      "source": [
        "Посмотрим глазами на вопросы и предсказанные БЕРТом ответы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-zfePRibQYD"
      },
      "outputs": [],
      "source": [
        "with open('output_nbest_file', 'r') as iofile:\n",
        "    predicted_answers = json.load(iofile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFoJvgIGbQYD"
      },
      "outputs": [],
      "source": [
        "questions = {}\n",
        "for paragraph in small_sample['data'][0]['paragraphs']:\n",
        "    for question in paragraph['qas']:\n",
        "        questions[question['id']] = {\n",
        "            'question': question['question'],\n",
        "            'answers': question['answers'],\n",
        "            'paragraph': paragraph['context']\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "ui_oHkdbbQYD"
      },
      "outputs": [],
      "source": [
        "for q_num, (key, data) in enumerate(predicted_answers.items()):\n",
        "    gt = '' if len(questions[key]['answers']) == 0 else questions[key]['answers'][0]['text']\n",
        "    print('Вопрос {0}:'.format(q_num+1), questions[key]['question'])\n",
        "    print('-----------------------------------')\n",
        "    print('Ground truth:', gt)\n",
        "    print('-----------------------------------')   \n",
        "    print('Ответы, предсказанные БЕРТом:')\n",
        "    preds = ['{0}) '.format(ans_num + 1) + answer['text'] + \\\n",
        "             ' (уверенность {0:.2f}%)'.format(answer['probability']*100) \\\n",
        "             for ans_num, answer in enumerate(data)]\n",
        "    print('\\n'.join(preds))\n",
        "#     print('-----------------------------------')   \n",
        "#     print('Параграф:', questions[key]['paragraph'])\n",
        "    print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K7qp1hibQYE"
      },
      "outputs": [],
      "source": [
        "!export SQUAD_DIR=/path/to/SQUAD\n",
        "\n",
        "python run_squad.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path bert-base-cased \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --train_file $SQUAD_DIR/train-v1.1.json \\\n",
        "  --predict_file $SQUAD_DIR/dev-v1.1.json \\\n",
        "  --per_gpu_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 2.0 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir /tmp/debug_squad/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r-MGiAjbQYE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "48px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "task10_bert_squad.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}