{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNKaJz5j_ylj"
      },
      "source": [
        "# Определение эмоциональной окраски твитов с помощью BERT\n",
        "\n",
        "Сегодня мы не будем кодить руками обучение BERT, а покажем, как, максимально просто, максимально быстро, найти готовый предобученный BERT и, минимальным количеством кода, дофайнтюнить его для решения вашей задачи."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL7XSN_e0hg6",
        "outputId": "8167b043-9d41-4c09-b6a8-eae706301b85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stepik-dl-nlp'...\n",
            "remote: Enumerating objects: 289, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 289 (delta 10), reused 14 (delta 6), pack-reused 266\u001b[K\n",
            "Receiving objects: 100% (289/289), 42.27 MiB | 10.58 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "Checking out files: 100% (51/51), done.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (1.0.2)\n",
            "Collecting spacy-udpipe\n",
            "  Downloading spacy_udpipe-1.0.0-py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (1.10.0+cu111)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.2.2)\n",
            "Collecting ipymarkup\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (4.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.62.3)\n",
            "Collecting youtokentome\n",
            "  Downloading youtokentome-1.0.6-cp37-cp37m-manylinux2010_x86_64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.11.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (4.10.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (5.5.0)\n",
            "Collecting pyconll\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 47.7 MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Collecting livelossplot==0.5.3\n",
            "  Downloading livelossplot-0.5.3-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (5.2.1)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.10.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.1.0)\n",
            "Collecting spacy<4.0.0,>=3.0.0\n",
            "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 36.1 MB/s \n",
            "\u001b[?25hCollecting ufal.udpipe>=1.2.0\n",
            "  Downloading ufal.udpipe-1.2.0.3.tar.gz (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 38.3 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 32.4 MB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.6)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.4.1)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.6)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (21.3)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 22.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (7.1.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 18.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.8.2)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.5)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (3.13)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (22.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.9.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.0)\n",
            "Building wheels for collected packages: ufal.udpipe, intervaltree, wget\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626646 sha256=6c607c05c89a6183e629bd57e771ea3f956451866439b8773d2958e924af5ac6\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/b5/8e/3da091629a21ce2d10bf90759d0cb034ba10a5cf7a01e83d64\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26119 sha256=8089c224bce8baaf2d877edb2fbdcecf9d36a251be3ce876dc84c3c16cb41234\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/85/bd/1001cbb46dcfb71c2001cd7401c6fb250392f22a81ce3722f7\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=f27c52f9b23650fa2759bfe32338fc9827149e10dc3a51f864c9a65c56bf123b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built ufal.udpipe intervaltree wget\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, ufal.udpipe, spacy, pymorphy2-dicts-ru, intervaltree, dawg-python, youtokentome, wget, spacy-udpipe, pymorphy2, pyconll, livelossplot, ipymarkup, gensim\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: intervaltree\n",
            "    Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed catalogue-2.0.6 dawg-python-0.7.2 gensim-3.8.1 intervaltree-3.1.0 ipymarkup-0.9.0 langcodes-3.3.0 livelossplot-0.5.3 pathy-0.6.1 pyconll-3.1.0 pydantic-1.8.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 spacy-udpipe-1.0.0 srsly-2.4.2 thinc-8.0.13 typer-0.4.0 ufal.udpipe-1.2.0.3 wget-3.2 youtokentome-1.0.6\n"
          ]
        }
      ],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle,\n",
        "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
        "\n",
        "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "# import sys; sys.path.append('./stepik-dl-nlp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV"
      },
      "source": [
        "## Установка библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NmMdkZO8R6q",
        "outputId": "9fc82948-b21d-4af8-cd3d-49ff5d205c2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.20.52-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 33.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.10.0+cu111)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.10.0.2)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting botocore<1.24.0,>=1.23.52\n",
            "  Downloading botocore-1.23.52-py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 38.3 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.1-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 21.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.52->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.52->boto3->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 37.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, pytorch-transformers\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.20.52 botocore-1.23.52 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.5.1 sacremoses-0.0.47 sentencepiece-0.1.96 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ok002ceNB8E7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_transformers import BertTokenizer, BertConfig\n",
        "from pytorch_transformers import AdamW, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYsV4H8fCpZ-",
        "outputId": "010f8072-268f-4c87-b239-175040f60df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla K80\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device == 'cpu':\n",
        "    print('cpu')\n",
        "else:\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    print(torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guw6ZNtaswKc"
      },
      "source": [
        "## Загрузка данных\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uol3IJ7o0hg-"
      },
      "source": [
        "Мы выбрали необычный датасет с разметкой сентимента русскоязычных твитов (подробнее про него в [статье](http://www.swsys.ru/index.php?page=article&id=3962&lang=)). В корпусе, который мы использовали 114,911 положительных и 111,923 отрицательных записей. Загрузить его можно [тут](https://study.mokoron.com/).\n",
        "\n",
        "* дата публикации;\n",
        "* имя автора;\n",
        "* текст твита;\n",
        "* класс, к которому принадлежит текст (положительный, отрицательный, нейтральный);\n",
        "* количество добавлений сообщения в избранное;\n",
        "* количество ретвитов (количество копирований этого сообщения другими пользователями);\n",
        "* количество друзей пользователя;\n",
        "* количество пользователей, у которых данный юзер в друзьях (количество фоловеров);\n",
        "* количество листов, в которых состоит пользователь.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ou9-WXJW0hg-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "pos_texts = pd.read_csv('./stepik-dl-nlp/datasets/bert_sentiment_analysis/positive.csv', encoding='utf8', sep=';', header=None)\n",
        "neg_texts = pd.read_csv('./stepik-dl-nlp/datasets/bert_sentiment_analysis/negative.csv', encoding='utf8', sep=';', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jwgkfYmk0hg_",
        "outputId": "89639b66-c565-4138-dcc3-f05192a11d75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1b255fd0-e05d-43fc-9f99-da3e1ab6ebc9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18000</th>\n",
              "      <td>409376525889204224</td>\n",
              "      <td>1386437944</td>\n",
              "      <td>RonyLiss</td>\n",
              "      <td>@kto_to__tam @mudak1999 @msdarren16 ага,наприм...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2878</td>\n",
              "      <td>289</td>\n",
              "      <td>429</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104190</th>\n",
              "      <td>411118922063032320</td>\n",
              "      <td>1386853363</td>\n",
              "      <td>adamantia88</td>\n",
              "      <td>Правду Настя сказала. Если буду все встречи на...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1912</td>\n",
              "      <td>54</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78419</th>\n",
              "      <td>410720433957515264</td>\n",
              "      <td>1386758356</td>\n",
              "      <td>rrrraaaaaaaaaa</td>\n",
              "      <td>Ахахахха, психанула на ваню, хотя он не мне ск...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>431</td>\n",
              "      <td>77</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85157</th>\n",
              "      <td>410792587763937280</td>\n",
              "      <td>1386775559</td>\n",
              "      <td>VAS_KALLAS</td>\n",
              "      <td>Игра \"найдите алкоголиков\" ахаххахахахааххахах...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4521</td>\n",
              "      <td>49</td>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31760</th>\n",
              "      <td>409743183023579136</td>\n",
              "      <td>1386525362</td>\n",
              "      <td>mmv93</td>\n",
              "      <td>@zarifulin_k Вот сразу видно: человек домашку ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5578</td>\n",
              "      <td>107</td>\n",
              "      <td>183</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b255fd0-e05d-43fc-9f99-da3e1ab6ebc9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b255fd0-e05d-43fc-9f99-da3e1ab6ebc9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b255fd0-e05d-43fc-9f99-da3e1ab6ebc9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                        0           1               2   ...   9    10  11\n",
              "18000   409376525889204224  1386437944        RonyLiss  ...  289  429   0\n",
              "104190  411118922063032320  1386853363     adamantia88  ...   54   30   0\n",
              "78419   410720433957515264  1386758356  rrrraaaaaaaaaa  ...   77  101   0\n",
              "85157   410792587763937280  1386775559      VAS_KALLAS  ...   49   55   0\n",
              "31760   409743183023579136  1386525362           mmv93  ...  107  183   0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "pos_texts.sample(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Теперь давайте приготовим наши данные так, чтобы их можно было подать в BERT для дообучения. Обратите внимание на специальные токены — \"CLS\" и \"SEP\", которые мы будем добавлять в начало и конец наших предложений. Как вы помните, именно такой формат нужен BERT для того, чтобы работать с входными предложениями"
      ],
      "metadata": {
        "id": "VHZYEZgN54wT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-aPEdvg30hhA"
      },
      "outputs": [],
      "source": [
        "sentences = np.concatenate([pos_texts[3].values, neg_texts[3].values])\n",
        "\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = [[1] for _ in range(pos_texts.shape[0])] + [[0] for _ in range(neg_texts.shape[0])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QpvNcTwi0hhA"
      },
      "outputs": [],
      "source": [
        "assert len(sentences) == len(labels) == pos_texts.shape[0] + neg_texts.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QicTMoO0hhA",
        "outputId": "95c2f979-613c-4e5b-d7ef-3eccdc537e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] Дим, ты помогаешь мне, я тебе, все взаимно, все правильно) [SEP]\n"
          ]
        }
      ],
      "source": [
        "print(sentences[1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9Eqjc0wj0hhB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_sentences, test_sentences, train_gt, test_gt = train_test_split(sentences, labels, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQLmsEI60hhB",
        "outputId": "9ab4fd35-49a6-428e-e0bd-ff8d1b81846a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "158783 68051\n"
          ]
        }
      ],
      "source": [
        "print(len(train_gt), len(test_gt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "## Inputs\n",
        "\n",
        "Теперь импортируем токенизатор для BERT, который превратит наши тексты в набор токенов, соответствующих тем, что встречались в словаре предобученной модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z474sSC6oe7A",
        "outputId": "dec00271-f9cf-4bfb-9113-c64ebaa046d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 316966.33B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '@', 'anne', '##t', '##20', '##14', '##1', 'б', '##л', '##а', '##г', '##о', '##р', '##о', '##д', '##н', '##ы', '##е', 'с', '##л', '##ов', '##а', 'и', 'б', '##л', '##а', '##г', '##о', '##р', '##о', '##д', '##н', '##ы', '##и', 'п', '##о', '##р', '##ы', '##в', '.', '.', 'н', '##е', 'з', '##на', '##ю', ',', 'р', '##е', '##з', '##у', '##л', '##ь', '##т', '##а', '##т', '##и', '##в', '##н', '##ы', '##и', 'л', '##и', '(', '(', '[SEP]']\n"
          ]
        }
      ],
      "source": [
        "from pytorch_transformers import BertTokenizer, BertConfig\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in train_sentences]\n",
        "print (tokenized_texts[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87_kXUeT2-br"
      },
      "source": [
        "BERTу нужно предоставить специальный формат входных данных.\n",
        "\n",
        "\n",
        "- **input ids**: последовательность чисел, отождествляющих каждый токен с его номером в словаре.\n",
        "- **labels**: вектор из нулей и единиц. В нашем случае нули обозначают негативную эмоциональную окраску, единицы - положительную.\n",
        "- **segment mask**: (необязательно) последовательность нулей и единиц, которая показывает, состоит ли входной текст из одного или двух предложений.Для случая одного предложения получится вектор из одних нулей. Для двух: <length_of_sent_1> нулей и <length_of_sent_2> единиц. \n",
        "(как я понял, это из-за того, что берт решает 2 задачи (маскинга и пред-ния след предлож) и тут мы показывает, что решать будем только одну из задач\n",
        "- **attention mask**: (необязательно) последовательность нулей и единиц, где единицы обозначают токены предложения, нули - паддинг. Паддинг нужен для того, чтобы BERT мог работать с предложениями разной длины. Более длинные предложения мы будем обрезать до 100 токенов, а для более коротких предложений использовать паддинг."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Cp9BPRd1tMIo"
      },
      "outputs": [],
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "input_ids = pad_sequences(\n",
        "    input_ids,\n",
        "    maxlen=100,\n",
        "    dtype=\"long\",\n",
        "    truncating=\"post\",\n",
        "    padding=\"post\"\n",
        ")\n",
        "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Хорошо, теперь нам нужно поделить наши данные на тренировку и валидацию."
      ],
      "metadata": {
        "id": "RO03jG9oJLef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aFbE-UHvsb7-"
      },
      "outputs": [],
      "source": [
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(\n",
        "    input_ids, train_gt, \n",
        "    random_state=42,\n",
        "    test_size=0.1\n",
        ")\n",
        "\n",
        "train_masks, validation_masks, _, _ = train_test_split(\n",
        "    attention_masks,\n",
        "    input_ids,\n",
        "    random_state=42,\n",
        "    test_size=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jw5K2A5Ko1RF"
      },
      "outputs": [],
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks = torch.tensor(train_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bh-rqfuj0hhD"
      },
      "outputs": [],
      "source": [
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дальше мы превращаем наши данные в pytorch тензоры, и давайте посмотрим на формат, в котором у нас лежат наши лэйблы. Обратите внимание на формат лейблов в наших данных. Мы подаём не просто list() из нулей и единиц, а мы подаём list(list()). Это нужно для того, чтобы поддерживать также возможность работы с задачами, где мы каждому объекту присваиваем несколько классов. Например, если бы мы хотели присваивать некоторые метки (лейблы) тем для наших предложений (ну, или скорее для наших текстов), то мы хотели бы уметь присваивать несколько тем одному тексту. "
      ],
      "metadata": {
        "id": "qlRWXA-FJvec"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPDFxAWA0hhD",
        "outputId": "598a5997-13af-4358-8b4f-ebcddf410582"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1],\n",
              "        [0],\n",
              "        [1],\n",
              "        ...,\n",
              "        [1],\n",
              "        [0],\n",
              "        [1]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Нужно создать итераторы с помощью DataLoader. Данные по батчам мы будем разбивать произвольно с помощью RandomSampler (здесь). Разбитие данных на батчи позволит нам более эффективно использовать память во время обучения, поскольку нам не придётся загружать весь датасет в память."
      ],
      "metadata": {
        "id": "LNH4OF8lKB2K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GEgLpFVlo1Z-"
      },
      "outputs": [],
      "source": [
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(\n",
        "    train_data,\n",
        "    sampler=RandomSampler(train_data),\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ULPWqgKq0hhE"
      },
      "outputs": [],
      "source": [
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_dataloader = DataLoader(\n",
        "    validation_data,\n",
        "    sampler=SequentialSampler(validation_data),\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNl8khAhPYju"
      },
      "source": [
        "## Обучение модели\n",
        "\n",
        "Для начала мы хотим изменить предобученный BERT так, чтобы он выдавал метки для классификации. А затем — дофайнтюнить полученную сеть на наших данных. Мы берём готовую модификацию BERT для классификации из pytorch-transformers, она называется \"BertForSequenceClassification\". Импортируем её. Это обычный BERT с добавленным одним линейным слоем для классификации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqvOsRvs0hhE"
      },
      "source": [
        "Загружаем [BertForSequenceClassification](https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py#L1129):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sQa_8040hhE"
      },
      "outputs": [],
      "source": [
        "from pytorch_transformers import AdamW, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ffAGXuZ0hhF"
      },
      "source": [
        "Аналогичные модели есть и для других задач:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmW2UpCm0hhF"
      },
      "outputs": [],
      "source": [
        "from pytorch_transformers import BertForQuestionAnswering, BertForTokenClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Теперь давайте чуть-чуть подробнее рассмотрим сам процесс файнтюнинга. Как мы помним, первый токен в каждом предложении в наших данных — это метка \"CLS\". Скрытое состояние, относящееся к этой метке, должно содержать в себе агрегированное представление всего предложения, которое дальше будет использоваться для классификации. Таким образом, когда мы скормили предложение в процессе обучения сети, выходом должен быть вектор со скрытым состоянием, относящийся к метке \"CLS\". Дополнительный полносвязный слой, который мы добавили, имеет размер [\"hidden state\", \"количество классов\"] — это двухмерный вектор. В нашем случае, количество классов равно \"2\", то есть, на выходе мы получим два числа, представляющих классы \"положительная эмоциональная окраска\", \"отрицательная эмоциональная окраска\". \n",
        "\n",
        "\n",
        " В целом, замораживание слоёв BERT обычно не сильно сказывается на итоговом качестве, однако стоит помнить о тех случаях, когда домен для предобучения и дообучения был разным. Например, когда мы предобучили нашу сеть на каких-то официальных текстах (на правовых актах или на научных статьях), а дообучаем её на, например, твитах, на неформальной лексике. В таких случаях лучше тренировать все слои сети, не замораживая ничего."
      ],
      "metadata": {
        "id": "ah26c3CcOGYm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFsCTp_mporB",
        "outputId": "c6fde951-4631-4e0f-9c62-d64545e2a2de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 433/433 [00:00<00:00, 245257.75B/s]\n",
            "100%|██████████| 440473133/440473133 [00:34<00:00, 12637369.47B/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отлично, теперь давайте обсудим гиперпараметры для обучения нашей модели. Авторы статьи про BERT советуют выбирать learning rate из следующего списка:  5 * 10^-5 , 3 * 10^-5, 2 * 10^-5 . А количество эпох делать не слишком большим (2 или 4 будет достаточно). Мы же пробуем дообучать нашу сеть за одну эпоху, а в качестве learning rate давайте выберем 2 * 10^-5\n",
        " "
      ],
      "metadata": {
        "id": "lO6Hs1ezPnIv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QxSMw0FrptiL"
      },
      "outputs": [],
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "6J-FYdx6nFE_",
        "outputId": "e693abf2-c41d-4b27-ec88-074caa5c8065"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f08ce431f2f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Обновляем параметры и делаем шаг используя посчитанные градиенты\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Будем сохранять loss во время обучения\n",
        "# и рисовать график в режиме реального времени\n",
        "train_loss_set = []\n",
        "train_loss = 0\n",
        "\n",
        "\n",
        "# Обучение\n",
        "# Переводим модель в training mode\n",
        "model.train()\n",
        "\n",
        "\n",
        "for step, batch in enumerate(train_dataloader):\n",
        "    # добавляем батч для вычисления на GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Распаковываем данные из dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # если не сделать .zero_grad(), градиенты будут накапливаться\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "    train_loss_set.append(loss[0].item())  \n",
        "    \n",
        "    # Backward pass\n",
        "    loss[0].backward()\n",
        "    \n",
        "    # Обновляем параметры и делаем шаг используя посчитанные градиенты\n",
        "    optimizer.step()\n",
        "\n",
        "    # Обновляем loss\n",
        "    train_loss += loss[0].item()\n",
        "    \n",
        "    # Рисуем график\n",
        "    clear_output(True)\n",
        "    plt.plot(train_loss_set)\n",
        "    plt.title(\"Training loss\")\n",
        "    plt.xlabel(\"Batch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "    \n",
        "print(\"Loss на обучающей выборке: {0:.5f}\".format(train_loss / len(train_dataloader)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Валидация\n",
        "# Переводим модель в evaluation mode\n",
        "model.eval()\n",
        "\n",
        "valid_preds, valid_labels = [], []\n",
        "\n",
        "for batch in validation_dataloader:   \n",
        "    # добавляем батч для вычисления на GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Распаковываем данные из dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # При использовании .no_grad() модель не будет считать и хранить градиенты.\n",
        "    # Это ускорит процесс предсказания меток для валидационных данных.\n",
        "    with torch.no_grad():\n",
        "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "    # Перемещаем logits и метки классов на CPU для дальнейшей работы\n",
        "    logits = logits[0].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    batch_preds = np.argmax(logits, axis=1)\n",
        "    batch_labels = np.concatenate(label_ids)     \n",
        "    valid_preds.extend(batch_preds)\n",
        "    valid_labels.extend(batch_labels)\n",
        "\n",
        "print(\"Процент правильных предсказаний на валидационной выборке: {0:.2f}%\".format(\n",
        "    accuracy_score(valid_labels, valid_preds) * 100\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKbHbum8R3y4",
        "outputId": "13e69d1e-b533-41f8-8b98-101fd0082027"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Процент правильных предсказаний на валидационной выборке: 96.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3"
      },
      "source": [
        "# Оценка качества на отложенной выборке"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mAN0LZBOOPVh"
      },
      "outputs": [],
      "source": [
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "input_ids = pad_sequences(\n",
        "    input_ids,\n",
        "    maxlen=100,\n",
        "    dtype=\"long\",\n",
        "    truncating=\"post\",\n",
        "    padding=\"post\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "zUVLMJPL0hhG"
      },
      "outputs": [],
      "source": [
        "attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
        "\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(test_gt)\n",
        "\n",
        "prediction_data = TensorDataset(\n",
        "    prediction_inputs,\n",
        "    prediction_masks,\n",
        "    prediction_labels\n",
        ")\n",
        "\n",
        "prediction_dataloader = DataLoader(\n",
        "    prediction_data, \n",
        "    sampler=SequentialSampler(prediction_data),\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hba10sXR7Xi6"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "    # добавляем батч для вычисления на GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Распаковываем данные из dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # При использовании .no_grad() модель не будет считать и хранить градиенты.\n",
        "    # Это ускорит процесс предсказания меток для тестовых данных.\n",
        "    with torch.no_grad():\n",
        "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "    # Перемещаем logits и метки классов на CPU для дальнейшей работы\n",
        "    logits = logits[0].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Сохраняем предсказанные классы и ground truth\n",
        "    batch_preds = np.argmax(logits, axis=1)\n",
        "    batch_labels = np.concatenate(label_ids)  \n",
        "    test_preds.extend(batch_preds)\n",
        "    test_labels.extend(batch_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeNriDOg0hhH"
      },
      "outputs": [],
      "source": [
        "acc_score = accuracy_score(test_labels, test_preds)\n",
        "print('Процент правильных предсказаний на отложенной выборке составил: {0:.2f}%'.format(\n",
        "    acc_score*100\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAHUt_AB0hhH"
      },
      "outputs": [],
      "source": [
        "print('Неправильных предсказаний: {0}/{1}'.format(\n",
        "    sum(test_labels != test_preds),\n",
        "    len(test_labels)\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы показали, что предобученный BERT может быстро (всего за одну эпоху) давать хорошее качество при решении задачи анализа эмоциональной окраски текстов. Кроме того, обратите внимание, что мы не тюнили параметры и использовали сравнительно маленький размеченный корпус, чтобы получить accuracy больше 98%. Тем не менее, если не делать дообучение под конкретную задачу вовсе, то получить хорошее качество вряд ли выйдет. Кроме того, на этом семинаре мы познакомились с библиотекой pytorch-transformers, которая позволяет использовать готовые обёртки над моделями, специально созданными для решения той или иной задачи. Использовать BERT при решении повседневных NLP задач совсем не трудно. Не нужно даже вручную скачивать веса модели, искать их где-то в интернете — библиотека абсолютно всё сделает за вас. Отбросив необходимость небольшой предобработки текстов, сложность применения предобученного BERT с использованием библиотеки pytorch-transformers оказывается не сильно больше, чем — ну, например, импортировать лог-регрессию из sk-learn, и примените её, а качество итоговое получается гораздо выше. Вы можете использовать предобученный BERT, GPT-2 или какие-то другие сети для решения других задач — не только классификации, но и чего-то более сложного, например, для решения задачи вопросно-ответного поиска, или, может быть, машинного перевода или выделения именованных сущностей. Единственное, что вам нужно будет сделать — это импортировать другую модель из pytorch-transformers и подготовить ваши данные для обучения в чуть-чуть другом формате. Успеха в дальнейшей работе с pytorch-transformers!"
      ],
      "metadata": {
        "id": "hEYGAa14S9iB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfmz6r_40hhH"
      },
      "source": [
        "### Домашнее задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pss7tHJX0hhH"
      },
      "source": [
        "Скачайте датасет с отзывами на фильмы. Например, используйте датасет [IMDB Dataset of 50K Movie Reviews](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re70NwiN0hhH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('datasets/bert_sentiment_analysis/homework/IMDB_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPJL4pAh0hhH",
        "outputId": "83ebd784-6f8b-41c0-b2c8-688303f99588"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KtvAoYR0hhI"
      },
      "source": [
        "Используйте для дообучения BERT датасет IMDB. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etsGFKMx0hhI"
      },
      "source": [
        "Ответьте на вопросы:\n",
        "1. удалось ли достичь такого же accuracy (98\\%) при использовании IMDB датасета?\n",
        "2. удалось ли получить хорошее качество классификации всего за одну эпоху?\n",
        "3. подумайте, в чем может быть причина различий в дообучении одной и той же модели на разных датасетах\n",
        "    - Внимательно изучите датасет с русскими твитами. В чем его особенности? Нет ли явных паттернов или ключевых слов, которые однозначно определяют сентимент твита?\n",
        "    - Попробуйте удалить пунктуацию из датасета с русскими твитами и перезапустите дообучение модели. Изменилось ли итоговое качество работы модели? Почему?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-A_41Lt0hhJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "9_bert_sentiment_analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}