{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Свёрточные нейросети и POS-теггинг\n",
    "\n",
    "POS-теггинг - определение частей речи (снятие частеречной неоднозначности)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stepik-dl-nlp'...\n",
      "remote: Enumerating objects: 289, done.\u001b[K\n",
      "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 289 (delta 10), reused 14 (delta 6), pack-reused 266\u001b[K\n",
      "Receiving objects: 100% (289/289), 42.27 MiB | 9.67 MiB/s, done.\n",
      "Resolving deltas: 100% (139/139), done.\n",
      "Requirement already satisfied: scikit-learn in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (0.23.1)\n",
      "Collecting spacy-udpipe\n",
      "  Downloading spacy_udpipe-1.0.0-py3-none-any.whl (11 kB)\n",
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 809 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.2 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (1.9.1)\n",
      "Requirement already satisfied: matplotlib in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.4.3)\n",
      "Collecting ipymarkup\n",
      "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: lxml in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (4.6.3)\n",
      "Requirement already satisfied: scipy in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.7.1)\n",
      "Requirement already satisfied: pandas in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (1.3.3)\n",
      "Requirement already satisfied: tqdm in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.59.0)\n",
      "Collecting youtokentome\n",
      "  Downloading youtokentome-1.0.6-cp38-cp38-macosx_10_14_x86_64.whl (164 kB)\n",
      "\u001b[K     |████████████████████████████████| 164 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: seaborn in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.11.1)\n",
      "Requirement already satisfied: ipykernel in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (5.3.4)\n",
      "Requirement already satisfied: ipython in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (7.22.0)\n",
      "Collecting pyconll\n",
      "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
      "Collecting gensim==3.8.1\n",
      "  Downloading gensim-3.8.1.tar.gz (23.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.4 MB 17.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Collecting livelossplot==0.5.3\n",
      "  Downloading livelossplot-0.5.3-py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.21.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.15.0)\n",
      "Collecting smart_open>=1.8.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 9.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: bokeh in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2.3.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.7.4.3)\n",
      "Requirement already satisfied: tornado>=4.2 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (6.1)\n",
      "Requirement already satisfied: appnope in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (6.1.12)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.0.5)\n",
      "Requirement already satisfied: pickleshare in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (52.0.0.post20210125)\n",
      "Requirement already satisfied: decorator in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (5.0.6)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.17.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (3.0.17)\n",
      "Requirement already satisfied: pygments in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.8.1)\n",
      "Requirement already satisfied: backcall in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from jedi>=0.16->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from pexpect>4.3->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from traitlets>=4.1.0->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.2.0)\n",
      "Requirement already satisfied: intervaltree>=3 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (3.1.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (8.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2021.1)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.2 MB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib>=0.11 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (2.1.0)\n",
      "Collecting ufal.udpipe>=1.2.0\n",
      "  Downloading ufal.udpipe-1.2.0.3-cp38-cp38-macosx_10_9_x86_64.whl (863 kB)\n",
      "\u001b[K     |████████████████████████████████| 863 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy<4.0.0,>=3.0.0\n",
      "  Downloading spacy-3.2.1-cp38-cp38-macosx_10_9_x86_64.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-macosx_10_9_x86_64.whl (31 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-macosx_10_9_x86_64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 14.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp38-cp38-macosx_10_9_x86_64.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp38-cp38-macosx_10_9_x86_64.whl (609 kB)\n",
      "\u001b[K     |████████████████████████████████| 609 kB 14.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp38-cp38-macosx_10_9_x86_64.whl (450 kB)\n",
      "\u001b[K     |████████████████████████████████| 450 kB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp38-cp38-macosx_10_9_x86_64.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp38-cp38-macosx_10_9_x86_64.whl (18 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.11.3)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (20.9)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2021.10.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (7.1.2)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (5.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.7.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (20.0.0)\n",
      "Building wheels for collected packages: gensim, docopt, wget\n",
      "  Building wheel for gensim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gensim: filename=gensim-3.8.1-cp38-cp38-macosx_10_9_x86_64.whl size=24205475 sha256=55969d642813006c94544ce96734eaf15e2ef4c607f3bf39f329cabd8a074835\n",
      "  Stored in directory: /Users/ila/Library/Caches/pip/wheels/33/de/03/7346ae70da7f980f78569668caf78fb2d678b176e549557c7d\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=885b60c1349c99a83ff3c0d79e98ab33d6f2e76471e9fe9fd7af9f6911ead99c\n",
      "  Stored in directory: /Users/ila/Library/Caches/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=67f43ea69b66471182304f5709a68fbfaaa5ee38575ab4dd2395101491467a83\n",
      "  Stored in directory: /Users/ila/Library/Caches/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built gensim docopt wget\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, ufal.udpipe, spacy, pymorphy2-dicts-ru, docopt, dawg-python, youtokentome, wget, spacy-udpipe, pymorphy2, pyconll, livelossplot, ipymarkup, gensim\n",
      "Successfully installed blis-0.7.5 catalogue-2.0.6 cymem-2.0.6 dawg-python-0.7.2 docopt-0.6.2 gensim-3.8.1 ipymarkup-0.9.0 langcodes-3.3.0 livelossplot-0.5.3 murmurhash-1.0.6 pathy-0.6.1 preshed-3.0.6 pyconll-3.1.0 pydantic-1.8.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 smart-open-5.2.1 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 spacy-udpipe-1.0.0 srsly-2.4.2 thinc-8.0.13 typer-0.4.0 ufal.udpipe-1.2.0.3 wasabi-0.9.0 wget-3.2 youtokentome-1.0.6\n"
     ]
    }
   ],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle,\n",
    "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
    "\n",
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "# import sys; sys.path.append('./stepik-dl-nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:42:57.976431Z",
     "start_time": "2019-10-29T19:42:57.959538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyconll in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: spacy_udpipe in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy_udpipe) (3.2.1)\n",
      "Requirement already satisfied: ufal.udpipe>=1.2.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy_udpipe) (1.2.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.9.0)\n",
      "Requirement already satisfied: jinja2 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (20.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.4.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.59.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (8.0.13)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.7.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.21.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.25.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (0.4.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.0.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from spacy<4.0.0,>=3.0.0->spacy_udpipe) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.0.0->spacy_udpipe) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_udpipe) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (2021.10.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_udpipe) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ila/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_udpipe) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyconll\n",
    "# !pip install spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:34.549739Z",
     "start_time": "2019-10-29T19:49:32.179692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import wget\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pyconll\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import dlnlputils\n",
    "from dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n",
    "    character_tokenize, pos_corpus_to_tensor, POSTagger\n",
    "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
    "\n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка текстов и разбиение на обучающую и тестовую подвыборки\n",
    "\n",
    "\"SynTag Rus\" Этот корпус был размечен руками, лингвистами, и в нём содержится разметка по частям речи, по нормальным формам слов, синтаксическая разметка. Он предназначен для того, чтобы настраивать и проверять методы лингвистического анализа текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'data'...\n",
      "remote: Enumerating objects: 533, done.\u001b[K\n",
      "remote: Total 533 (delta 0), reused 0 (delta 0), pack-reused 533\u001b[K\n",
      "Receiving objects: 100% (533/533), 310.96 MiB | 5.50 MiB/s, done.\n",
      "Resolving deltas: 100% (351/351), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/UniversalDependencies/UD_Russian-SynTagRus.git data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "формате CoNLL — это достаточно распространённый формат для того, чтобы хранить аннотированные деревья и разную лингвистическую разметку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.525561Z",
     "start_time": "2019-10-29T19:49:37.315213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "full_train = pyconll.load_from_file('data/ru_syntagrus-ud-train-a.conllu')\n",
    "full_test = pyconll.load_from_file('data/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.548127Z",
     "start_time": "2019-10-29T19:49:56.527559Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:56.916262Z",
     "start_time": "2019-10-29T19:49:56.549806Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:57.251433Z",
     "start_time": "2019-10-29T19:49:56.919818Z"
    }
   },
   "outputs": [],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.124148Z",
     "start_time": "2019-10-29T19:49:57.254191Z"
    }
   },
   "outputs": [],
   "source": [
    "train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n",
    "char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\n",
    "print(\"Количество уникальных символов\", len(char_vocab))\n",
    "print(list(char_vocab.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.524125Z",
     "start_time": "2019-10-29T19:49:58.125577Z"
    }
   },
   "outputs": [],
   "source": [
    "UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n",
    "label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.752672Z",
     "start_time": "2019-10-29T19:49:58.526431Z"
    }
   },
   "outputs": [],
   "source": [
    "train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "train_dataset = TensorDataset(train_inputs, train_labels)\n",
    "\n",
    "test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "test_dataset = TensorDataset(test_inputs, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.754883Z",
     "start_time": "2019-10-29T19:49:40.582Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_inputs[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:49:58.756496Z",
     "start_time": "2019-10-29T19:49:40.711Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательная свёрточная архитектура"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.316516Z",
     "start_time": "2019-10-29T19:46:17.539Z"
    }
   },
   "outputs": [],
   "source": [
    "class StackedConv1d(nn.Module):\n",
    "    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(layers_n):\n",
    "            layers.append(nn.Sequential(\n",
    "                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.LeakyReLU()))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание частей речи на уровне отдельных токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.317452Z",
     "start_time": "2019-10-29T19:46:23.135Z"
    }
   },
   "outputs": [],
   "source": [
    "class SingleTokenPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n",
    "        super().__init__()\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.backbone = StackedConv1d(embedding_size, **kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Linear(embedding_size, labels_num)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "        \n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        \n",
    "        features = self.backbone(char_embeddings)\n",
    "        \n",
    "        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "        \n",
    "        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n",
    "        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n",
    "        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.318497Z",
     "start_time": "2019-10-29T19:46:23.764Z"
    }
   },
   "outputs": [],
   "source": [
    "single_token_model = SingleTokenPOSTagger(len(char_vocab), len(label2id), embedding_size=64, layers_n=3, kernel_size=3, dropout=0.3)\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in single_token_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.319470Z",
     "start_time": "2019-10-29T19:46:25.552Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_single_token_model) = train_eval_loop(single_token_model,\n",
    "                                            train_dataset,\n",
    "                                            test_dataset,\n",
    "                                            F.cross_entropy,\n",
    "                                            lr=5e-3,\n",
    "                                            epoch_n=10,\n",
    "                                            batch_size=64,\n",
    "                                            device='cuda',\n",
    "                                            early_stopping_patience=5,\n",
    "                                            max_batches_per_epoch_train=500,\n",
    "                                            max_batches_per_epoch_val=100,\n",
    "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                       factor=0.5,\n",
    "                                                                                                                       verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.320568Z",
     "start_time": "2019-10-29T19:46:47.579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_single_token_model.state_dict(), './models/single_token_pos.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.321566Z",
     "start_time": "2019-10-29T19:46:47.731Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "single_token_model.load_state_dict(torch.load('./models/single_token_pos.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.324276Z",
     "start_time": "2019-10-29T19:46:48.445Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred = predict_with_model(single_token_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(single_token_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предсказание частей речи на уровне предложений (с учётом контекста)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.325744Z",
     "start_time": "2019-10-29T19:46:50.139Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentenceLevelPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n",
    "        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n",
    "        self.labels_num = labels_num\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
    "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
    "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
    "        \n",
    "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
    "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
    "        char_features = self.single_token_backbone(char_embeddings)\n",
    "        \n",
    "        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
    "\n",
    "        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n",
    "        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n",
    "\n",
    "        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.326925Z",
     "start_time": "2019-10-29T19:46:50.310Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n",
    "                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T19:47:48.327888Z",
     "start_time": "2019-10-29T19:46:50.737Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_sentence_level_model) = train_eval_loop(sentence_level_model,\n",
    "                                              train_dataset,\n",
    "                                              test_dataset,\n",
    "                                              F.cross_entropy,\n",
    "                                              lr=5e-3,\n",
    "                                              epoch_n=10,\n",
    "                                              batch_size=64,\n",
    "                                              device='cuda',\n",
    "                                              early_stopping_patience=5,\n",
    "                                              max_batches_per_epoch_train=500,\n",
    "                                              max_batches_per_epoch_val=100,\n",
    "                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                         factor=0.5,\n",
    "                                                                                                                         verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:16.542052Z",
     "start_time": "2019-08-29T13:56:16.529110Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_sentence_level_model.state_dict(), './models/sentence_level_pos.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:16.564926Z",
     "start_time": "2019-08-29T13:56:16.544481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "sentence_level_model.load_state_dict(torch.load('./models/sentence_level_pos.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.092139Z",
     "start_time": "2019-08-29T13:56:16.567242Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred = predict_with_model(sentence_level_model, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(sentence_level_model, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применение полученных теггеров и сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.105418Z",
     "start_time": "2019-08-29T13:56:42.093744Z"
    }
   },
   "outputs": [],
   "source": [
    "single_token_pos_tagger = POSTagger(single_token_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
    "sentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.125540Z",
     "start_time": "2019-08-29T13:56:42.106771Z"
    }
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'Мама мыла раму.',\n",
    "    'Косил косой косой косой.',\n",
    "    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n",
    "    'Сяпала Калуша с Калушатами по напушке.',\n",
    "    'Пирожки поставлены в печь, мама любит печь.',\n",
    "    'Ведро дало течь, вода стала течь.',\n",
    "    'Три да три, будет дырка.',\n",
    "    'Три да три, будет шесть.',\n",
    "    'Сорок сорок'\n",
    "]\n",
    "test_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.148124Z",
     "start_time": "2019-08-29T13:56:42.126930Z"
    }
   },
   "outputs": [],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.168810Z",
     "start_time": "2019-08-29T13:56:42.149698Z"
    }
   },
   "outputs": [],
   "source": [
    "for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n",
    "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свёрточный модуль своими руками"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.193140Z",
     "start_time": "2019-08-29T13:56:42.170233Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n",
    "                                   requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n",
    "\n",
    "        batch_size, src_channels, sequence_len = x.shape        \n",
    "        if self.padding > 0:\n",
    "            pad = x.new_zeros(batch_size, src_channels, self.padding)\n",
    "            x = torch.cat((pad, x, pad), dim=-1)\n",
    "            sequence_len = x.shape[-1]\n",
    "\n",
    "        chunks = []\n",
    "        chunk_size = sequence_len - self.kernel_size + 1\n",
    "        for offset in range(self.kernel_size):\n",
    "            chunks.append(x[:, :, offset:offset + chunk_size])\n",
    "\n",
    "        in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n",
    "        in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n",
    "        out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n",
    "        out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n",
    "        return out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T13:56:42.210013Z",
     "start_time": "2019-08-29T13:56:42.194620Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
    "                                                      single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d),\n",
    "                                                      context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d))\n",
    "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T14:06:00.233326Z",
     "start_time": "2019-08-29T13:56:42.211456Z"
    }
   },
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_sentence_level_model_my_conv) = train_eval_loop(sentence_level_model_my_conv,\n",
    "                                                      train_dataset,\n",
    "                                                      test_dataset,\n",
    "                                                      F.cross_entropy,\n",
    "                                                      lr=5e-3,\n",
    "                                                      epoch_n=10,\n",
    "                                                      batch_size=64,\n",
    "                                                      device='cuda',\n",
    "                                                      early_stopping_patience=5,\n",
    "                                                      max_batches_per_epoch_train=500,\n",
    "                                                      max_batches_per_epoch_val=100,\n",
    "                                                      lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
    "                                                                                                                                 factor=0.5,\n",
    "                                                                                                                                 verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-29T14:06:39.145214Z",
     "start_time": "2019-08-29T14:06:00.234936Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pred = predict_with_model(best_sentence_level_model_my_conv, train_dataset)\n",
    "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
    "                             torch.tensor(train_labels))\n",
    "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
    "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
    "print()\n",
    "\n",
    "test_pred = predict_with_model(best_sentence_level_model_my_conv, test_dataset)\n",
    "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
    "                            torch.tensor(test_labels))\n",
    "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
    "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
