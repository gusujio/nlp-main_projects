{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p5ne17YU7rn"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdbxrwFeU7ro",
        "outputId": "e027d5d1-86f3-4091-ed3f-541b0a4d7a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stepik-dl-nlp'...\n",
            "remote: Enumerating objects: 289, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 289 (delta 10), reused 14 (delta 6), pack-reused 266\u001b[K\n",
            "Receiving objects: 100% (289/289), 42.27 MiB | 21.23 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (1.0.2)\n",
            "Collecting spacy-udpipe\n",
            "  Downloading spacy_udpipe-1.0.0-py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (1.10.0+cu111)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.2.2)\n",
            "Collecting ipymarkup\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (4.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.62.3)\n",
            "Collecting youtokentome\n",
            "  Downloading youtokentome-1.0.6-cp37-cp37m-manylinux2010_x86_64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 64.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.11.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (4.10.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (5.5.0)\n",
            "Collecting pyconll\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Collecting gensim==3.8.1\n",
            "  Downloading gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 1.6 MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Collecting livelossplot==0.5.3\n",
            "  Downloading livelossplot-0.5.3-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.15.0)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.10.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (3.0.0)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "  Downloading ufal.udpipe-1.2.0.3.tar.gz (304 kB)\n",
            "\u001b[K     |████████████████████████████████| 304 kB 62.2 MB/s \n",
            "\u001b[?25hCollecting spacy<4.0.0,>=3.0.0\n",
            "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 50.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (57.4.0)\n",
            "Collecting thinc<8.1.0,>=8.0.12\n",
            "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
            "\u001b[K     |████████████████████████████████| 628 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.11.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.6)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 34.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.6)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (21.3)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
            "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 61.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.6)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (7.1.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.8.2)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.5)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (3.13)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (22.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.9.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.0)\n",
            "Building wheels for collected packages: ufal.udpipe, intervaltree, wget\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626617 sha256=489fb7aac383e737ef931ebc2267467cda5388eda4ce5da2362db428856cdb37\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/b5/8e/3da091629a21ce2d10bf90759d0cb034ba10a5cf7a01e83d64\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26119 sha256=a7d9c4d21d5202d3b629559fdacf1dcaa9b509b5d56545d7e3723f0060edc5c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/85/bd/1001cbb46dcfb71c2001cd7401c6fb250392f22a81ce3722f7\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=10339b9d2d44aeaf8d9d1370d66ecf6174ff823f732d17a9fee9038ffc1eec26\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built ufal.udpipe intervaltree wget\n",
            "Installing collected packages: catalogue, typer, srsly, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, ufal.udpipe, spacy, pymorphy2-dicts-ru, intervaltree, dawg-python, youtokentome, wget, spacy-udpipe, pymorphy2, pyconll, livelossplot, ipymarkup, gensim\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Attempting uninstall: intervaltree\n",
            "    Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed catalogue-2.0.6 dawg-python-0.7.2 gensim-3.8.1 intervaltree-3.1.0 ipymarkup-0.9.0 langcodes-3.3.0 livelossplot-0.5.3 pathy-0.6.1 pyconll-3.1.0 pydantic-1.8.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 spacy-udpipe-1.0.0 srsly-2.4.2 thinc-8.0.13 typer-0.4.0 ufal.udpipe-1.2.0.3 wget-3.2 youtokentome-1.0.6\n"
          ]
        }
      ],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle,\n",
        "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
        "\n",
        "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "# import sys; sys.path.append('./stepik-dl-nlp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:30.785285Z",
          "start_time": "2019-10-29T19:19:29.542846Z"
        },
        "id": "CdZFbr8XU7rp"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import dlnlputils\n",
        "from dlnlputils.data import tokenize_corpus, build_vocabulary, texts_to_token_ids, \\\n",
        "    PaddedSequenceDataset, Embeddings\n",
        "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
        "from dlnlputils.visualization import plot_vectors\n",
        "\n",
        "init_random_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA8KIa9VU7rq"
      },
      "source": [
        "## Загрузка данных и подготовка корпуса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:31.270503Z",
          "start_time": "2019-10-29T19:19:30.787789Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbSsSj5lU7rq",
        "outputId": "8151d150-c599-4a91-9c79-8c1ca48b66fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обучающая выборка 125344\n",
            "Тестовая выборка 53719\n",
            "\n",
            "1/4 cup sour cream\n",
            "10 ounces swordfish, red snapper or other firm-fleshed fish\n",
            "1 tablespoon minced basil leaves\n",
            "Handful fresh parsley, finely minced\n",
            "4 ounces lard or butter, plus more for brushing tops\n",
            "4 to 5 green cardamom pods\n",
            "1 stick ( 1/4 pound) unsalted butter, softened\n",
            "1/4 teaspoon red pepper flakes, preferably Turkish or Aleppo (see note), more to taste\n",
            "1 tablespoon fresh lemon juice\n",
            "1/4 cup scallions, thinly sliced\n"
          ]
        }
      ],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "# подгрузили данные для обучения\n",
        "full_dataset = list(pd.read_csv('./stepik-dl-nlp/datasets/nyt-ingredients-snapshot-2015.csv')['input'].dropna())\n",
        "random.shuffle(full_dataset)\n",
        "# разбили их на train и test\n",
        "TRAIN_VAL_SPLIT = int(len(full_dataset) * 0.7)\n",
        "train_source = full_dataset[:TRAIN_VAL_SPLIT]\n",
        "test_source = full_dataset[TRAIN_VAL_SPLIT:]\n",
        "print(\"Обучающая выборка\", len(train_source))\n",
        "print(\"Тестовая выборка\", len(test_source))\n",
        "print()\n",
        "print('\\n'.join(train_source[:10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.137838Z",
          "start_time": "2019-10-29T19:19:31.272363Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLO4Yk-EU7rr",
        "outputId": "7b28708c-3863-49cf-abf5-9c01508660b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sour cream\n",
            "ounces swordfish snapper other firm fleshed fish\n",
            "tablespoon minced basil leaves\n",
            "handful fresh parsley finely minced\n",
            "ounces lard butter plus more brushing tops\n",
            "green cardamom pods\n",
            "stick pound unsalted butter softened\n",
            "teaspoon pepper flakes preferably turkish aleppo note more taste\n",
            "tablespoon fresh lemon juice\n",
            "scallions thinly sliced\n"
          ]
        }
      ],
      "source": [
        "# токенизируем\n",
        "train_tokenized = tokenize_corpus(train_source)\n",
        "test_tokenized = tokenize_corpus(test_source)\n",
        "print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.325205Z",
          "start_time": "2019-10-29T19:19:32.140837Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leb0tBTuU7rr",
        "outputId": "3aba684c-20e1-44d6-ffa7-0404e4cd96b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер словаря 2267\n",
            "[('<PAD>', 0), ('tablespoons', 1), ('teaspoon', 2), ('chopped', 3), ('salt', 4), ('pepper', 5), ('cups', 6), ('ground', 7), ('fresh', 8), ('tablespoon', 9)]\n"
          ]
        }
      ],
      "source": [
        "# строим словарь и отбрасываем редкие и частые слова\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=0.9, min_count=5, pad_word='<PAD>')\n",
        "print(\"Размер словаря\", len(vocabulary))\n",
        "print(list(vocabulary.items())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.686258Z",
          "start_time": "2019-10-29T19:19:32.327711Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moNrlN3CU7rr",
        "outputId": "a711cba3-50b7-40ec-aa56-5922a9f4a3bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "222 52\n",
            "22 878 574 127 246 707 181\n",
            "9 19 88 33\n",
            "517 8 43 15 19\n",
            "22 586 20 45 47 648 649\n",
            "59 329 535\n",
            "200 12 50 20 266\n",
            "2 5 140 78 1208 735 153 47 10\n",
            "9 8 31 25\n",
            "98 65 27\n"
          ]
        }
      ],
      "source": [
        "# отображаем в номера токенов\n",
        "# токены меняем на их номера\n",
        "train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n",
        "test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n",
        "\n",
        "print('\\n'.join(' '.join(str(t) for t in sent)\n",
        "                for sent in train_token_ids[:10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.967989Z",
          "start_time": "2019-10-29T19:19:32.688319Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "50GMmNebU7rr",
        "outputId": "92dcd8cc-2900-41cf-9163-d5a6190ec404"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.hist([len(s) for s in train_token_ids], bins=20);\n",
        "plt.title('Гистограмма длин предложений');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.001487Z",
          "start_time": "2019-10-29T19:19:32.970153Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUtvYkYCU7rs",
        "outputId": "d4eff5ca-8404-4a2a-8454-6704844e7384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([222,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0]), tensor(0))\n"
          ]
        }
      ],
      "source": [
        "MAX_SENTENCE_LEN = 20\n",
        "\"\"\"\n",
        "для того, что бы мы могли хранить предложения в тензорах (те считать на видео карте),\n",
        "они должны быть одинаковой длины \n",
        "если они больше MAX_SENTENCE_LEN, то значит обрежим кол-во слов в предложении \n",
        "если их мало, то дополним нулями \n",
        "\"\"\"\n",
        "train_dataset = PaddedSequenceDataset(train_token_ids,\n",
        "                                      np.zeros(len(train_token_ids)),\n",
        "                                      out_len=MAX_SENTENCE_LEN)\n",
        "test_dataset = PaddedSequenceDataset(test_token_ids,\n",
        "                                     np.zeros(len(test_token_ids)),\n",
        "                                     out_len=MAX_SENTENCE_LEN)\n",
        "print(train_dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55MVOGXUU7rs"
      },
      "source": [
        "## Алгоритм обучения - Skip Gram Negative Sampling\n",
        "\n",
        "Основная идея, в модели SkipGram. В модели SkipGram для каждого слова у нас есть два вектора — первый вектор используется, когда слово находится в центре скользящего окна, и второй вектор используется, когда это слово описывает контекст. Для того, чтобы хранить эти вектора, мы создаём две матрицы. Каждая матрица имеет размер по количеству слов в словаре и по количеству элементов в векторе. То есть — размер словаря на размер нашего эмбеддинга. Настраивать значения этих матриц, то есть обучать модель, мы будем по методу максимального правдоподобия. Изначальная интуиция описывается вот этой формулой — то есть мы моделируем условное распределение соседних слов в некотором окне (небольшом, как правило это \"5\") при условии того, что мы пронаблюдали центральное слово и у нас есть какие-то параметры модели. Это распределение в общем виде посчитать очень сложно. Мы предполагаем, что оно раскладывается на произведения более простых распределений. Каждое такое \"более простое\" распределение нам говорит, насколько вероятно можно встретить какое-то контекстное слово рядом с центральным словом. То есть, для слов, которые типично встречаются рядом, наша модель должна возвращать вероятность, близкую к единице, а для слов, которые никогда не встречаются рядом — вероятность, близкую к нулю. \n",
        "\n",
        "**Skip Gram** - предсказываем соседние слова по центральному слову\n",
        "\n",
        "**Negative Sampling** - аппроксимация softmax\n",
        "\n",
        "$$ W, D \\in \\mathbb{R}^{Vocab \\times EmbSize} $$\n",
        "\n",
        "$$ \\sum_{CenterW_i} P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) \\rightarrow \\max_{W,D} $$\n",
        "\n",
        "$$ P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) = \\prod_j P(CtxW_j | CenterW_i; W, D) $$\n",
        "    \n",
        "$$ P(CtxW_j | CenterW_i; W, D) = \\frac{e^{w_i \\cdot d_j}} { \\sum_{j=1}^{|V|} e^{w_i \\cdot d_j}} = softmax \\simeq \\frac{e^{w_i \\cdot d_j^+}} { \\sum_{j=1}^{k} e^{w_i \\cdot d_j^-}}, \\quad k \\ll |V| $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.065376Z",
          "start_time": "2019-10-29T19:19:33.003081Z"
        },
        "id": "0oCUu-DqU7rs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "184a18e9-ddce-4a42-9988-6646efe114cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 1., 1., 1., 0., 0., 0.],\n",
              "        [0., 1., 1., 1., 0., 1., 1., 1., 0., 0.],\n",
              "        [0., 0., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
              "        [0., 0., 0., 1., 1., 1., 0., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 1., 1., 1., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 1., 1., 1., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def make_diag_mask(size, radius):\n",
        "    \"\"\"Квадратная матрица размера Size x Size с двумя полосами ширины radius вдоль главной диагонали\"\"\"\n",
        "    idxs = torch.arange(size)\n",
        "    abs_idx_diff = (idxs.unsqueeze(0) - idxs.unsqueeze(1)).abs()\n",
        "    mask = ((abs_idx_diff <= radius) & (abs_idx_diff > 0)).float()\n",
        "    return mask\n",
        "\n",
        "make_diag_mask(10, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG7ePgPuU7rt"
      },
      "source": [
        "**Negative Sampling** работает следующим образом - мы **максимизируем сумму вероятностей двух событий**: \n",
        "\n",
        "* \"этот пример центрального слова вместе с контекстными словами взят **из тренировочной выборки**\": $$ P(y=1 | CenterW_i; CtxW_j) = sigmoid(w_i \\cdot d_j) = \\frac{1}{1+e^{-w_i \\cdot d_j}} $$\n",
        "\n",
        "$$ \\\\ $$\n",
        "\n",
        "* \"этот пример центрального слова вместе со случайми контекстными словами **выдуман** \": $$ P(y=0 | CenterW_i; CtxW_{noise}) = 1 - P(y=1 | CenterW_i;  CtxW_{noise}) = \\frac{1}{1+e^{w_i \\cdot d_{noise}}} $$\n",
        "\n",
        "$$ \\\\ $$\n",
        "\n",
        "$$ NEG(CtxW_j, CenterW_i) = log(\\frac{1}{1+e^{-w_i \\cdot d_j}}) + \\sum_{l=1}^{k}log(\\frac{1}{1+e^{w_i \\cdot d_{noise_l}}})  \\rightarrow \\max_{W,D} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.101379Z",
          "start_time": "2019-10-29T19:19:33.068154Z"
        },
        "id": "5tOrfVAxU7rt"
      },
      "outputs": [],
      "source": [
        "class SkipGramNegativeSamplingTrainer(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, sentence_len, radius=5, negative_samples_n=5):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.negative_samples_n = negative_samples_n\n",
        "\n",
        "        self.center_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n",
        "        self.center_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
        "        self.center_emb.weight.data[0] = 0\n",
        "\n",
        "        self.context_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)        \n",
        "        self.context_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
        "        self.context_emb.weight.data[0] = 0\n",
        "\n",
        "        self.positive_sim_mask = make_diag_mask(sentence_len, radius)\n",
        "    \n",
        "    def forward(self, sentences):\n",
        "        \"\"\"sentences - Batch x MaxSentLength - идентификаторы токенов\"\"\"\n",
        "        batch_size = sentences.shape[0]\n",
        "        center_embeddings = self.center_emb(sentences)  # Batch x MaxSentLength x EmbSize\n",
        "\n",
        "        # оценить сходство с настоящими соседними словами\n",
        "        positive_context_embs = self.context_emb(sentences).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n",
        "        positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x MaxSentLength x MaxSentLength\n",
        "        positive_probs = torch.sigmoid(positive_sims)\n",
        "\n",
        "        # увеличить оценку вероятности встретить эти пары слов вместе\n",
        "        positive_mask = self.positive_sim_mask.to(positive_sims.device)\n",
        "        positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n",
        "                                               positive_mask.expand_as(positive_probs))\n",
        "\n",
        "        # выбрать случайные \"отрицательные\" слова\n",
        "        negative_words = torch.randint(1, self.vocab_size,\n",
        "                                       size=(batch_size, self.negative_samples_n),\n",
        "                                       device=sentences.device)  # Batch x NegSamplesN\n",
        "        negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n",
        "        negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x MaxSentLength x NegSamplesN\n",
        "        \n",
        "        # уменьшить оценку вероятность встретить эти пары слов вместе\n",
        "        negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n",
        "                                                           negative_sims.new_zeros(negative_sims.shape))\n",
        "\n",
        "        return positive_loss + negative_loss\n",
        "\n",
        "\n",
        "def no_loss(pred, target):\n",
        "    \"\"\"Фиктивная функция потерь - когда модель сама считает функцию потерь\"\"\"\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFgb9AefU7ru"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.130307Z",
          "start_time": "2019-10-29T19:19:33.103036Z"
        },
        "id": "fR-s0_grU7ru"
      },
      "outputs": [],
      "source": [
        "trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), 100, MAX_SENTENCE_LEN,\n",
        "                                          radius=5, negative_samples_n=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.830221Z",
          "start_time": "2019-10-29T19:19:33.132062Z"
        },
        "scrolled": false,
        "id": "_NIbSsR1U7rv"
      },
      "outputs": [],
      "source": [
        "best_val_loss, best_model = train_eval_loop(trainer,\n",
        "                                            train_dataset,\n",
        "                                            test_dataset,\n",
        "                                            no_loss,\n",
        "                                            lr=1e-2,\n",
        "                                            epoch_n=2,\n",
        "                                            batch_size=8,\n",
        "                                            device='cpu',\n",
        "                                            early_stopping_patience=10,\n",
        "                                            max_batches_per_epoch_train=2000,\n",
        "                                            max_batches_per_epoch_val=len(test_dataset),\n",
        "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=1, verbose=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.862018Z",
          "start_time": "2019-10-29T19:20:12.832046Z"
        },
        "id": "5jzgRy0OU7rv"
      },
      "outputs": [],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "torch.save(trainer.state_dict(), 'models/sgns.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.888270Z",
          "start_time": "2019-10-29T19:20:12.864706Z"
        },
        "id": "ud9ktDpAU7rv"
      },
      "outputs": [],
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "trainer.load_state_dict(torch.load('models/sgns.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr19KFPRU7rv"
      },
      "source": [
        "## Исследуем характеристики полученных векторов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.919904Z",
          "start_time": "2019-10-29T19:20:12.890671Z"
        },
        "id": "ZwynQTB2U7rw"
      },
      "outputs": [],
      "source": [
        "embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.942708Z",
          "start_time": "2019-10-29T19:20:12.921619Z"
        },
        "id": "eJoeVNitU7rw"
      },
      "outputs": [],
      "source": [
        "embeddings.most_similar('chicken')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.965936Z",
          "start_time": "2019-10-29T19:20:12.944423Z"
        },
        "id": "cFlieP1PU7rw"
      },
      "outputs": [],
      "source": [
        "embeddings.analogy('cake', 'cacao', 'cheese')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.991060Z",
          "start_time": "2019-10-29T19:20:12.967532Z"
        },
        "id": "I0BBIQfVU7rw"
      },
      "outputs": [],
      "source": [
        "test_words = ['salad', 'fish', 'salmon', 'sauvignon', 'beef', 'pork', 'steak', 'beer', 'cake', 'coffee', 'sausage', 'wine', 'merlot', 'zinfandel', 'trout', 'chardonnay', 'champagne', 'cacao']\n",
        "test_vectors = embeddings.get_vectors(*test_words)\n",
        "print(test_vectors.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:13.318676Z",
          "start_time": "2019-10-29T19:20:12.996595Z"
        },
        "id": "P7Ni3XT8U7rw"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "plot_vectors(test_vectors, test_words, how='svd', ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CaW8cnSU7rx"
      },
      "source": [
        "## Обучение Word2Vec с помощью Gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:13.613797Z",
          "start_time": "2019-10-29T19:20:13.321353Z"
        },
        "id": "Ity9vdkbU7rx"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.075005Z",
          "start_time": "2019-10-29T19:20:13.615729Z"
        },
        "id": "_U9NMoR3U7rx"
      },
      "outputs": [],
      "source": [
        "word2vec = gensim.models.Word2Vec(sentences=train_tokenized, size=100,\n",
        "                                  window=5, min_count=5, workers=4,\n",
        "                                  sg=1, iter=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.109583Z",
          "start_time": "2019-10-29T19:20:17.076599Z"
        },
        "id": "O5ED1MT1U7rx"
      },
      "outputs": [],
      "source": [
        "word2vec.wv.most_similar('chicken')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.176357Z",
          "start_time": "2019-10-29T19:20:17.112948Z"
        },
        "id": "3yygONnWU7rx"
      },
      "outputs": [],
      "source": [
        "gensim_words = [w for w in test_words if w in word2vec.wv.vocab]\n",
        "gensim_vectors = np.stack([word2vec.wv[w] for w in gensim_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.428874Z",
          "start_time": "2019-10-29T19:20:17.179311Z"
        },
        "id": "IwbMtNdeU7rx"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "plot_vectors(gensim_vectors, test_words, how='svd', ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUuBgxQQU7rx"
      },
      "source": [
        "## Загрузка предобученного Word2Vec\n",
        "\n",
        "Источники готовых векторов:\n",
        "\n",
        "https://rusvectores.org/ru/ - для русского языка\n",
        "\n",
        "https://wikipedia2vec.github.io/wikipedia2vec/pretrained/ - много разных языков"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.460133Z",
          "start_time": "2019-10-29T19:20:17.430563Z"
        },
        "id": "1A3uH626U7ry"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.980509Z",
          "start_time": "2019-10-29T19:20:17.462239Z"
        },
        "id": "xiQF4ogbU7ry"
      },
      "outputs": [],
      "source": [
        "available_models = api.info()['models'].keys()\n",
        "print('\\n'.join(available_models))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.649035Z",
          "start_time": "2019-10-29T19:20:17.984118Z"
        },
        "scrolled": false,
        "id": "wcSacObtU7ry"
      },
      "outputs": [],
      "source": [
        "pretrained = api.load('word2vec-google-news-300')  # > 1.5 GB!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.651388Z",
          "start_time": "2019-10-29T19:19:29.817Z"
        },
        "id": "AwcRtmqCU7ry"
      },
      "outputs": [],
      "source": [
        "pretrained.most_similar('cheese')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.652649Z",
          "start_time": "2019-10-29T19:19:29.820Z"
        },
        "id": "hy_YgTnlU7rz"
      },
      "outputs": [],
      "source": [
        "pretrained.most_similar(positive=['man', 'queen'], negative=['king'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.653584Z",
          "start_time": "2019-10-29T19:19:29.823Z"
        },
        "id": "ygj60OjwU7rz"
      },
      "outputs": [],
      "source": [
        "pretrained_words = [w for w in test_words if w in pretrained.vocab]\n",
        "pretrained_vectors = np.stack([pretrained[w] for w in pretrained_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.654594Z",
          "start_time": "2019-10-29T19:19:29.828Z"
        },
        "id": "Hu50PbINU7rz"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "plot_vectors(pretrained_vectors, test_words, how='svd', ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voL-Gek7U7rz"
      },
      "source": [
        "## Заключение\n",
        "\n",
        "* Реализовали Skip Gram Negative Sampling на PyTorch\n",
        "* Обучили на корпусе рецептов\n",
        "    * Сходство слов модель выучила неплохо\n",
        "    * Для аналогий мало данных\n",
        "* Обучили SGNS с помощью библиотеки Gensim\n",
        "* Загрузили веса Word2Vec, полученные с помощью большого корпуса (GoogleNews)\n",
        "    * Списки похожих слов отличаются!\n",
        "    * Аналогии работают"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIC3-qcYU7rz"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "task2_word_embeddings.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}